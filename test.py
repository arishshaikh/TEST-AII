import os
import pandas as pd
import streamlit as st
import requests
from dotenv import load_dotenv

# Load environment variables from a .env file
load_dotenv()

# Set your Hugging Face API key from an environment variable
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY")

# Function to generate insights using GPT-J via Hugging Face's Inference API
def generate_insight(prompt):
    API_URL = "https://api-inference.huggingface.co/models/EleutherAI/gpt-j-6B"
    headers = {
        "Authorization": f"Bearer {HUGGINGFACE_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": 300,
            "temperature": 0.7,
            "top_p": 0.9,
            "do_sample": True
        }
    }
    
    try:
        response = requests.post(API_URL, headers=headers, json=payload)
        response.raise_for_status()  # Raises stored HTTPError, if one occurred.
    except requests.exceptions.HTTPError as http_err:
        return f"HTTP error occurred: {http_err} - {response.text}"
    except Exception as err:
        return f"An error occurred: {err}"
    
    # Parse the response
    try:
        result = response.json()
        # GPT-J's response may return 'generated_text' or a list of outputs
        if isinstance(result, list) and 'generated_text' in result[0]:
            return result[0]['generated_text']
        elif 'generated_text' in result:
            return result['generated_text']
        else:
            return "Unexpected response format from the API."
    except (KeyError, IndexError, ValueError) as e:
        return f"Error parsing response: {e}"

# Load the dataset from a CSV file
def load_data(file_path):
    try:
        data = pd.read_csv(file_path)
        return data
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return None

# Streamlit app
def main():
    st.title("Chocolate Sales Dashboard")
    st.subheader("AI-Generated Insights with GPT-J")

    # Preload the CSV file instead of file uploader
    csv_file_path = "C:/gpt project/cocoa.csv"  # Replace with your actual file path
    if os.path.exists(csv_file_path):
        # Load data
        data = load_data(csv_file_path)
        if data is not None:
            st.write("üìä **Dataset Preview:**", data)

            # Sidebar data range (if applicable)
            st.sidebar.subheader("Dashboard Navigation")
            date_range = st.sidebar.date_input("Select Date Range", [])

            # Generate insights
            if st.button("Generate AI Insights"):
                # Check if required columns exist
                required_columns = ['Product Name', 'Region', 'Profit']
                missing_columns = [col for col in required_columns if col not in data.columns]
                if missing_columns:
                    st.error(f"Dataset is missing the following required columns: {missing_columns}")
                    return

                product_names = ', '.join(data['Product Name'].dropna().unique())
                regions = ', '.join(data['Region'].dropna().unique())
                total_profit = f"${data['Profit'].sum():,.2f}"

                prompt = (
                    f"Here is the summary of the chocolate sales data:\n\n"
                    f"- **Products:** {product_names}\n"
                    f"- **Regions:** {regions}\n"
                    f"- **Total Profit:** {total_profit}\n\n"
                    f"Please provide insights on the best-selling products, regional performance, and recommendations to improve sales."
                )

                with st.spinner("Generating insights..."):
                    insights = generate_insight(prompt)

                st.subheader("üîç **Key Insights Generated by GPT-J**")
                st.write(insights)
    else:
        st.error(f"File not found: {csv_file_path}")

if __name__ == '__main__':
    main()
